[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Kray Freestone, a GIS Web Application Developer at North Point Geographic Solutions and a recent graduate of UW-Madison’s Masters Program in GIS Development.\nYou can see my resume here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Census Find\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMap Challenge 2021 & 2022\n\n\n\n\n\n\n\ncartography\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nJohn Ball Zoo Explorer\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVarieties of Democracy\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThe Tour de Iberia\n\n\n\n\n\n\n\ncartography\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTanaka Contours on Mars\n\n\n\n\n\n\n\ncartography\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMapping Gondor and Mordor\n\n\n\n\n\n\n\ncartography\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing Democratic Backsliding\n\n\n\n\n\n\n\ncartography\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing the Riot on the Capitol\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGenerating Running Loops with Graph Theory\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMapping Code Violations in Grand Rapids\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\nDemographic Analysis with Python\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2018\n\n\n\n\n\n\n  \n\n\n\n\nCity of Grand Rapids Cemeteries\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2018\n\n\n\n\n\n\n  \n\n\n\n\nGeocoding with Python\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2018\n\n\n\n\n\n\n  \n\n\n\n\nMesopotamian Marshes\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/census-find/index.html",
    "href": "posts/census-find/index.html",
    "title": "Census Find",
    "section": "",
    "text": "This application, Census Find, intends to make access to census data both accessible and reproducible. While there are other tools out there that aim to accomplish this, I believe this tool fills a special niche that did not, up until this point, exist. The intended audience are casual users who are not used to exploring census data, nor joining in census data with TIGER shapefiles to make cartographic products."
  },
  {
    "objectID": "posts/census-find/index.html#exploring",
    "href": "posts/census-find/index.html#exploring",
    "title": "Census Find",
    "section": "Exploring",
    "text": "Exploring\nData exploration is the first page a user is shown. On the left-hand side, a user can select whether they would like to explore states, counties, places, or census tracts. Depending on their choice, they may have to further filter their results by state. Next, a user can specifically search for a geography by name or GEOID. The search filters results in a table on the right-hand side.\nThe user also must apply a template to the geography they are querying. If none are available, they can be created in the Template tab. Upon clicking a geometry on the right side of the screen, the user will get taken to a page showing the data they requested.\nTouches like searching by GEOID, or having the data page create a reproducible URL – so they can go straight back to their data page without having to go through the explore page – are features I hope advanced and repeat users appreciate.\nIn the future, I would like to stylize the tables to be able to hide and show margin of errors, collapse table groups, and give warning messages to users when margin of error percentages are too high for useful analysis."
  },
  {
    "objectID": "posts/census-find/index.html#templates",
    "href": "posts/census-find/index.html#templates",
    "title": "Census Find",
    "section": "Templates",
    "text": "Templates\nThis page is what sets this product apart from others. Here, users can choose whether to build an ACS or Decennial template.\nAfter choosing a year for their data (currently, users cannot mix years to compare how a certain variable has changed through time), a user can search for and click on variables to add to their templates. Variables can be searched by label or variable name (e.g. P001001).\nAfter a user has clicked on all of the variables they want, they can give the template a name and click \"Finish\". Then, they will be able to see their template on the Explore page.\nCurrently, this page is missing some checks in place to make sure users do not mix variable years and ACS and Decennial data types. In the long-term, it would be nice if a user could mix years and survey (ACS or Decennial) types within one template. The database structure can already accommodate this, but the back-end and front-end are not ready."
  },
  {
    "objectID": "posts/census-find/index.html#query",
    "href": "posts/census-find/index.html#query",
    "title": "Census Find",
    "section": "Query",
    "text": "Query\nThis page lets users find geometries that match a certain criteria, such as, how many counties in Michigan have more than 70% of the population living in urban areas? Once a query is complete, the user is taken to a map where they can view the specific data they requested by hovering over a geography.\nUsers have an option of stacking multiple queries and specifying whether all of the criteria needs to be met, or any of the criteria needs to be met. Obviously, this does not give the user as much flexibility as a SQL statement, but hopefully it is enough for casual and advanced users alike.\nSimilar to the templates page, there are no checks to make sure a user is not submitting a query that mixes ACS and Decennial data, and years.\nMoreover, I would like to let the user query by margin of error percentage, so they could filter out results whose margin of error is too high for useful analysis. This would only apply to ACS data, since Decennial data consists of values and not estimates."
  },
  {
    "objectID": "posts/census-find/index.html#react-front-end",
    "href": "posts/census-find/index.html#react-front-end",
    "title": "Census Find",
    "section": "React Front-end",
    "text": "React Front-end\nAs the name states, the front-end was built in React. The following libraries are mainly responsible for front-end functionality:\n\nChakra UI is responsible for all HTML elements, layouts, typefaces, and colors. It is also responsible for the button that switches between light and dark modes.\nreact-map-gl is responsible for visualizing the spatial data. The library gives the option of using MapBox GL JS and Maplibre GL JS. I am using the latter.\naxios is responsible for fetching data from the back-end."
  },
  {
    "objectID": "posts/census-find/index.html#r-back-end",
    "href": "posts/census-find/index.html#r-back-end",
    "title": "Census Find",
    "section": "R Back-end",
    "text": "R Back-end\nThe backend is where most of the computation happens in this application. It is responsible for fetching variables and geometries from the Postgres database, as well as fetching new data from the census API. The back-end would not be possible without the following:\n\nTidycensus by Kyle Walker is an incredibly elegant library that allows users to fetch a host of census data, only some of which I exposed in the front-end. It provides easy methods to retrieve data and provides advanced calculations for apportioning non-standard geometries to census geometries (said calculations are not currently used in this application).\nPlumber is responsible for the back-end API. The library made building an API quite fast.\nThe tidyverse is a host of packages in R that is intended for working with data, whether that is data preparation or analysis. This was used to prepare data from Tidycensus before I sent it to the front-end."
  },
  {
    "objectID": "posts/census-find/index.html#postgres-database-with-postgis-enabled",
    "href": "posts/census-find/index.html#postgres-database-with-postgis-enabled",
    "title": "Census Find",
    "section": "Postgres Database with PostGIS Enabled",
    "text": "Postgres Database with PostGIS Enabled\nThis is responsible for hosting all census variables and geometries. The database is populated by scripts in R so that the base deployment can be reproduced anywhere. PostGIS is only used to store the geometries which are then returned through the R package sf."
  },
  {
    "objectID": "posts/city-of-grand-rapids-cemeteries/index.html",
    "href": "posts/city-of-grand-rapids-cemeteries/index.html",
    "title": "City of Grand Rapids Cemeteries",
    "section": "",
    "text": "The purpose of this project was to allow any citizen to view cemeteries online so that they can search for occupants in any of Grand Rapids’ six cemeteries. This project was based on four phases: implementing surveying maps, reconciling polygon data with cemetery records, automating much of the workflow in Python, and visualizing it all in ArcGIS Online.\n\n\n\nSurvey Map of Woodlawn West\n\n\nThe first phase of the project dealt largely with georeferencing the surveying maps, converting the image to editable features, and cleaning up said features. Though there are six cemeteries, there were eight different maps, each of which being georeferenced to their respective cemetery. After georeferencing, the survey maps were converted into editable features via the Raster to Polygon tool. From there, over 20,000 polygons from eight maps were cleaned up and labeled (e.g. deleting the numbers that were turned into polygons, inserting “blocks” and “lots” metadata into each polygon).\nThe second phase consisted of reconciling the polygons, which contained the bare metadata of “cemetery”, “block”, and “lot”, with the richer data of a spreadsheet derived from the city’s cemetery database. This proved to somewhat of a challenge given that there were multiple occupants per lot, for the survey’s smallest unit was not a grave but a lot. With multiple occupants per lot, there were only two options: a many-to-one relate or building a query table in order to do a many-to-one join. I chose the latter. While relating the information would be easiest and is possible in ArcGIS Online, the service’s online search function did not support data from relates, therefore, I had to build a query table.\nIt should be noted that there are some inherent flaws when working with surveying maps from the mid to late 1900s. Some data that existed in the up-to-date cemetery database was not reflected in the surveying maps, therefore, some lots — and subsequently occupants — do not appear in the final project. Moreover, if certain lots appeared in the surveying maps but did not appear in the spreadsheet, the query table operation then deleted empty polygons. Interestingly, even though the map appears to have fewer polygons, there are actually more: in a many-to-one query table join, polygons are stacked on top of each other when sharing the same space.\n\n\n\nBefore Query Table Join\n\n\n\n\n\nAfter Query Table Join\n\n\nFor the third phase, I automated most of the workflow with Python. At a high level, the program:\n\nCreates a one-to-many join via a query table with the cemetery spreadsheet and the cemetery shapefile\nSplits the query table result into eight different shapefiles (if we use only one shapefile for the web application, it would have to search through approximately 100,000 features; the maximum amount it can handle is around 20,000 features)\nUploads the shapefiles to ArcGIS Online\nCreates Tile Layers from the shapefiles in ArcGIS Online (the Tile Layers help visualize the data more easily in the web applications)\n\nNote: the file paths were removed from the following code.\nFirst, I create the query table from the shapefile and the spreadsheet and export the query table as its own shapefile name queryTable.\n\nNext, the query table needs to be split into eight different parts (the end result of this project is eight different web applications for their respective cemeteries).\n\nThe ArcPy library (used above) that comes with ArcGIS for Desktop is built on Python 2.7. The ArcGIS API for Python (which allows me to easily manipulate files in ArcGIS Online) is built on Python 3.5 (and above). So, I started a new script for this different library. The end function should overwrite shapefiles and create new tiles layers in ArcGIS Online.\nThe following function searches for shapefiles, feature services, and map services (tile layers) that match the cemetery’s name. If there is a match, they will be deleted.\n\nAfter the files are deleted, the program uploads new shapefiles and creates tile layers from them.\n\nFinally, the function is called and passed a list of shapefiles.\n\nThe fourth phase consists of making web maps and configuring web applications from the data that was uploaded.\nYou can find the full list of the cemeteries on the City of Grand Rapids Cemeteries page under Genealogy Information, and if you want to test out searching through one of the cemeteries, click here."
  },
  {
    "objectID": "posts/demographic-analysis-with-python/index.html",
    "href": "posts/demographic-analysis-with-python/index.html",
    "title": "Demographic Analysis with Python",
    "section": "",
    "text": "Before Query Table Join\n\n\nThe goal of this project is to analyze the demographics of donors to a non-profit in West Michigan with Python. Demographic data was acquired from the U.S. Census Bureau’s American Fact Finder, donor data through my geocoding project, and the shapefiles were found through the Census’s TIGER shapefiles. I looked for the following data:\n\nMedian Household Income in the Past 12 Months (a widely accepted measure of income)\nAge and Gender\nEducational Attainment\n\nFirst, I imported the relevant modules and set the current working directory:\n\nNext, the spreadsheets are turned into a pandas dataframe and the column headers are dropped.\n\nThere are only certain columns that I am interested in for each spreadsheet. All other columns are dropped through this function (columns were determined manually).\n\nAfter the demographic data is formatted correctly, it can be joined with the shapefile of census tracts.\n\nIn order to perform demographic analysis on the non-profit donors, the spreadsheet must be turned into a geopandas GeoDataFrame. This is done by reading in the spreadsheet, dropping any rows that were unsuccessfully geocoded, and then converting those columns into a Shapely Point object. Now that the dataframe has a correctly formatted geometry column, it can be converted into a GeoDataFrame.\n\nThe donors and demographics can then be spatially joined into one GeoDataFrame. The end result will be donors in Michigan (this is where a large majority of the donors are).\n\n\nANALYSIS\nI used the pygal library to make the following graphs. These graphs give a general impression of the non-profit donors and should be weighted according to where the donors are located. For example, if a large share of donors live in Ottawa County, their information will be weighted more heavily than the one donor in Oscoda County.\n\nIncome Groups\n\nEducational Attainment in Ages 25 and Over\n\nFinally, a heatmap is made with the Python library folium. This library allows Leaflet webmaps to be made from Python."
  },
  {
    "objectID": "posts/generating-running-loops-with-graph-theory/index.html",
    "href": "posts/generating-running-loops-with-graph-theory/index.html",
    "title": "Generating Running Loops with Graph Theory",
    "section": "",
    "text": "Warning\n\n\n\nDue to Heroku ending their free-tier, this website is no longer live. I hope to find another host for this website.\n\n\n\nLoopMyRun\nIf you want to skip reading, you can navigate straight to LoopMyRun. The purpose of this web app is to create a running (or walking) loop in any road network, given a preferred distance and location.\n\n\n\nLooking for a 5k in Madison, WI\n\n\nThough I wish I knew that Strava had already created this solution before I embarked on this project, creating this app did have worthwhile benefits anyhow:\n\nIt was a great learning experience.\nMy app is free while Strava’s routing is a premium feature.\nWhile Strava pulls potential routes from its 3 billion activities and 50 million users, my app does not keep track of your location, or email, or anything.\n\nThis is not to say mine is better by any means. Theirs is certainly more intelligent thanks to user data and you get more features. Mine is simply different.\n\n\nGraph Theory\nI am not going to sit here and pretend I know much about mathematics, but Graph Theory, and the graphing library JGraphtT for Java, was pivotal for making this application work.\n\n\n\nSample Graph\n\n\nReally, a road network is a lot like an undirected graph (though I suppose it could be directed if you are a car on a one-way). All intersections and roads can be plotted as nodes and edges respectively.\n\n\n\nPhoto by Hanson Lu on Unsplash\n\n\nMoreover, each one of those roads can be weighted based on what type of road they are. This app set the lowest weight to residential roads and the highest weights to primary roads.\n\n\nThe Data\nAll of the road data is retrieved from OpenStreetMap’s Overpass API. The program then transforms the roads and road vertices to graph nodes and edges.\n\n\nThe Stack\nThe back-end was built with Java and used Embedded Tomcat (credit to Heroku for getting me started with a nice template).\nThe front-end is pretty standard HTML/CSS/JavaScript. The mapping is done with Leaflet, and the distances and mile markers are calculated with turf.js.\nSince some of the calculations can take a while, when a user submits the form, the back-end will calculate possible routes and submit it to a temporary PostgresSQL database. When it is ready, the route will be sent from the database back to the user.\nSimilarly, saved routes are sent to a separate table in the database for long-term storage. I have a limit of 10,000 rows, so if the number gets to high, the oldest routes are deleted. As long as no-one abuses this, storage can remain free, and users will not need to be validated (neither of which I want to do) I would sooner just remove the save route feature).\n\n\nThe Code\nBreaking down the code here would be pretty boring and complicated, so if you want to delve into the code yourself, you can find it at my GitHub repository. Thanks for reading."
  },
  {
    "objectID": "posts/geocoding-with-python/index.html",
    "href": "posts/geocoding-with-python/index.html",
    "title": "Geocoding with Python",
    "section": "",
    "text": "The goal of this project was to map out the donors of a nonprofit in West Michigan. The CSV of donors numbered over 10,000 records. While there are many geocoding options out there, I decided to turn to an organization that specializes in data gathering for residences: the U.S. Census Bureau.\nFirst, I formatted the CSV for geocoding via Python’s pandas library:\n\nThe maximum amount the U.S. Census Bureau can geocode per file is 10,000. Since I was working with a CSV that slightly exceeded that, I needed to split the CSV in two. While there is probably a more elegant and flexible way to program this, the following should suffice:\n\nNow that I have files the census geocoder can handle, I can plug it into a function. This function will take the CSV as an input, and it will output a formatted CSV with additional information, such as latitude, longitude, match accuracy and more.\n\nEach function call, which geocodes around 5,000 addresses, takes around an hour and a half.\n\nFORMATTING & RESULTS\nThere was only one glaring error produced by the API that had to be fixed by hand. If an address had an apartment or suite number, it would put that suite number in an extra cell, thereby ruining the uniformity of the CSV. This was easily fixable by hand.\n\n\n\nThe bottom row is the desired format, the other five are skewed\n\n\nSince I had to check and fix the results by hand, I put the rest of the code into a second script. To reformat all of this data, I concatenated the two CSVs back together:\n\nOut of the 11,016 rows of data: - 9,374 were Matches (85.1%) - 935 were Ties (8.5%) - 703 were No Matches (6.4%)\nOut of those matches, 80.4% were exact, and 19.6% were non-exact. Match accuracies hovering around 85% seem to be adequate, though there is always room for improvement. It should be noted that some of the Ties and No Matches are due to some donors not providing data as to where they live. Moreover, this can explain how there are a total of 11,012 results (amongst match, tie, and no-match) from 11,016 rows of data.\nThe geocoding output produces 14 additional columns to my CSV, four of which I needed: longitude, latitude, match type, and match accuracy. First I dropped superfluous columns, kept the unique ID, and then concatenated the two outputs. The API’s documentation isn’t fantastic, so the meaning of the match type, Tie, is not entirely clear. However, it is clear enough that both Tie and No_Match do not produce coordinates.\n\n\n\nNext I stripped the CSV of any unnecessary data before merging it with the original document. Since the original document is only lacking coordinates and match types, I can drop everything but those four fields (latitude, longitude, match type, and match accuracy). In the next step, I’ll need to merge this output with the original spreadsheet, therefore, values must be ordered based on its index so the spreadsheets will match up.\n\nNow it is ready to merge with the original spreadsheet:\n\nWith all data acquired, the spreadsheet just needs to be cleaned up and reformatted.\n\nAnd with this, the final product can be uploaded to CARTO. The latitude and longitude do not need to be converted to a shapefile; CARTO simply recognizes those columns and converts it to something mappable.\n\n\n\nDonors throughout the continental United States\n\n\n\n\n\nExact (grey) and non-exact (blue) matches in the Michigan area"
  },
  {
    "objectID": "posts/john-ball-zoo-explorer/index.html",
    "href": "posts/john-ball-zoo-explorer/index.html",
    "title": "John Ball Zoo Explorer",
    "section": "",
    "text": "Warning\n\n\n\nDue to Heroku ending their free-tier, this website is no longer live. I hope to find another host for this website."
  },
  {
    "objectID": "posts/john-ball-zoo-explorer/index.html#introduction-and-overview",
    "href": "posts/john-ball-zoo-explorer/index.html#introduction-and-overview",
    "title": "John Ball Zoo Explorer",
    "section": "Introduction and Overview",
    "text": "Introduction and Overview\nThe goal of this project was to create a mobile-friendly web application that showcased a park, real or fictitious. I chose to use John Ball Zoo in Grand Rapids, MI.\nThis application allows users to click on exhibits, and then click on an animal's \"card\" to view extra information, such as seeing a larger picture (credits for each image is included), their Wikipedia article, and their conservation status. Each card also lets the user submit a rating (on a five-star scale). Once there are ratings on an animal, the user can view the break down of all ratings, and the card will update the average score.\nThe application URL is below, though it should be noted that in production, the SVG symbols for the facilities do not show up properly in Firefox. Please use Chrome or Edge instead.\nhttps://freestok.herokuapp.com/john-ball-zoo/"
  },
  {
    "objectID": "posts/john-ball-zoo-explorer/index.html#user-stories",
    "href": "posts/john-ball-zoo-explorer/index.html#user-stories",
    "title": "John Ball Zoo Explorer",
    "section": "User Stories",
    "text": "User Stories\nI was planning on this application to be used by the public. The user I had in mind had a goal of exploring the zoo easily. They were not researchers, nor were they advanced users that required advanced workflows to view animals at the zoo. Therefore, the imagined person is a casual user, wanting to take the least amount of clicks possible to find information on animals at the zoo, as well as facilities and buildings that are nearby.\nThe UI is built to make everything easy, painless, and to require as few clicks as possible. Moreover, feedback is included in the application so that user knows what the purpose of each interaction is."
  },
  {
    "objectID": "posts/john-ball-zoo-explorer/index.html#application-architecture",
    "href": "posts/john-ball-zoo-explorer/index.html#application-architecture",
    "title": "John Ball Zoo Explorer",
    "section": "Application Architecture",
    "text": "Application Architecture\nTechnically, since the application is hosted through Heroku, it treats the entire application as one front-end. In reality, which is reflected in the GitHub repository, the front-end uses React and the back-end uses Flask. Routing has to match between the two in order to work.\nTo expand on that, Heroku is looking for a python project. Flask runs, and it serves up a single index.html file, which is the output of the react-scripts build process. When running this application locally, one must open two terminals, one running \"npm start\" and the other running \"npm run server\". It also assumes that the terminal running \"npm run server\" is in a proper conda environment that has all the dependencies installed (which should be found in the project's root level requirements.txt file).\nAside from serving the assets, the back-end also handles the requests to get and set ratings. If it receives a GET request, it returns the specific animal's rating, and if it receives a POST request, it will create a rating for the animal. I did not spend the time to implement robust state tracking, so once a larger card is closed, a user could simply reopen it and re-submit a rating. However, while they are in the card, it no longer gives them the option to submit another rating.\nOn the React side of the application, the UI is split into two main parts: the map and the sidebar. There is a navbar as well, but it simply hosts a link to the GitHub repository and shows a help dialog box to give the user an idea about the application.\nThe sidebar contains animal \"cards\" which give the user information about each animal. It also hosts the ratings functionality. They are each broken into multiple components:\n\nSidebar\n\nCard container\n\nCard accordion\n\nSmall Card\nExpanded Card\n\nFive star rating\n\nFive star modal (showing the graph)\n\n\n\n\n\nMap\n\nWidgets (home, layer list, legend, search)"
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html",
    "href": "posts/map-challenge-2021-22/index.html",
    "title": "Map Challenge 2021 & 2022",
    "section": "",
    "text": "I have been meaning to compile these maps for a while, so here is a compilation of my maps for the #30DayMapChallenge (years 2021 and 2022)."
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-5---openstreetmap",
    "href": "posts/map-challenge-2021-22/index.html#day-5---openstreetmap",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 5 - OpenStreetMap",
    "text": "Day 5 - OpenStreetMap\n\n\n\nElectric London"
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-8---blue",
    "href": "posts/map-challenge-2021-22/index.html#day-8---blue",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 8 - Blue",
    "text": "Day 8 - Blue\n\n\n\nThe Great Lakes (with relative depths)"
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-9---monochrome",
    "href": "posts/map-challenge-2021-22/index.html#day-9---monochrome",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 9 - Monochrome",
    "text": "Day 9 - Monochrome\n\n\n\nKasei Valles, Mars"
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-11---3d",
    "href": "posts/map-challenge-2021-22/index.html#day-11---3d",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 11 - 3D",
    "text": "Day 11 - 3D\n\n\n\nElevation Ridgline Plot of Montserrat, Spain\n\n\nMade in Blender. It’s 3D, I promise."
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-17---land",
    "href": "posts/map-challenge-2021-22/index.html#day-17---land",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 17 - Land",
    "text": "Day 17 - Land\n\n\n\nRim to Rim trail of the Grand Canyon (Bright Angel excluded)"
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-19---islands",
    "href": "posts/map-challenge-2021-22/index.html#day-19---islands",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 19 - Island(s)",
    "text": "Day 19 - Island(s)\n\n\n\nPlan oblique view of Haleakala National Park on Maui\n\n\nThanks to John Nelson for his tutorial."
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-21---elevation",
    "href": "posts/map-challenge-2021-22/index.html#day-21---elevation",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 21 - Elevation",
    "text": "Day 21 - Elevation\n\n\n\nAnother view of Montserrat in Spain\n\n\nPlayed around with a more densified joy plot and controlling the emission by z-value."
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-9---space",
    "href": "posts/map-challenge-2021-22/index.html#day-9---space",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 9 - Space",
    "text": "Day 9 - Space\n\n\n\nTears in the Urban Fabric: Parking Lots in Downtown Grand Rapids\n\n\nMy urban planning professor in college called them tears in the urban fabric, so I’m showing them as voids in the felt style made by John Nelson."
  },
  {
    "objectID": "posts/map-challenge-2021-22/index.html#day-14---hexagons",
    "href": "posts/map-challenge-2021-22/index.html#day-14---hexagons",
    "title": "Map Challenge 2021 & 2022",
    "section": "Day 14 - Hexagons",
    "text": "Day 14 - Hexagons\n All “Muppet” place names in the continental US (e.g. Kermit, Gonzo, Animal, Janice, Pepe, Ms. Piggy, Rizzo, Scooter, Statler & Waldorf)\nData source: Geographic Names from USGS"
  },
  {
    "objectID": "posts/mapping-code-violations-in-grand-rapids/index.html",
    "href": "posts/mapping-code-violations-in-grand-rapids/index.html",
    "title": "Mapping Code Violations in Grand Rapids",
    "section": "",
    "text": "Warning\n\n\n\nDue to Heroku ending their free-tier, this website is no longer live. I hope to find another host for this website.\n\n\nThe City of Grand Rapids, with data available since 2004, has handled code compliance and building cases originating internally and from the public. At the time of this writing, the number of cases started is fast approaching 300,000 cases.\nInterestingly enough, there is no means to view this data spatially. That is why I built a mapping interface. I aim to update this data at least once a week.\nIf you would like to learn about the implementation of this web app, please read on. If not, then you can find the Code Compliance map here. Please be patient as the data loads.\nNote: this does not work on Android and probably does not work on iOS. Please use your computer. It is likely too much data for your phone to handle, so your mobile browser will crash.\n\nData Preparation\nFirst, I needed parcel data for the City of Grand Rapids so I could later join the case data with parcels by a shared attribute: their parcel number. Data was acquired from Kent County’s excellent data library.\n\n\n<img src=\"before-clip.png\" alt=\"Snow\" style=\"width:100%\">\n\n\n<img src=\"after-clip.png\" alt=\"Forest\" style=\"width:100%\">\n\n\n\nGrand Rapids before and after clipping\n\n\nThe parcels, as shown in the left image above, needed to be clipped to the City’s actually boundary. This was accomplished via Python’s geopandas library. Moreover, the script below exports the centroid of each parcel since I will want to represent each case in the middle of a parcel.\n\n\n\nData Scraping and Upload\nAll of the data in this project are open to the public, it is just not in an easily consumed format (e.g. a spreadsheet). Well, technically you can search for all properties on the City’s Accela portal, but you will have no luck trying to download all the results (the spreadsheet is too large).\nThat is why I turned to scraping the data via Python. More specifically, I utilized Selenium to enter date values and download results.\nThe original script broke down queries by month increments, from 2004 to 2020, and downloaded the results. To download ~300,000 records in this manner took 30 minutes. It would have taken much longer without utilizing multiprocessing.\nSubsequent runs of the script simply start where the last run of the script left off. With every new run, older cases are updated, data is cleaned, and scraped data are joined with the parcel centroid. All of this data are then manually uploaded to AWS S3, and a stripped down version is uploaded to CARTO (which I will talk more about later).\nYou can view both my scraping script and data cleaning script on GitHub.\n\n\nMapping Interface\nThe mapping interface was built primarily with Flask and Mapbox GL JS. The project layout for Flask was inspired by Miguel Grinberg’s Flask Mega-Tutorial.\nFlask is used for serving up web pages and returning data from user POST requests (e.g. updating status filters if the case type changes, querying PostgreSQL to see if a report is done, making a request for a report).\n\n\n\nBefore Query Table Join\n\n\nMapbox GL JS does the work of visualizing the data. Moreover, it is responsible for the case counts, filtering, and pop-ups.\nI won’t go into detail on all of the code (you can view this project on GitHub), but the popups are populated by querying rendered features, determining which points are inside the polygon with turf.js, and then inserting that array of cases into the pop-up.\n\n\n\nHosting the Web Application\nThis web application is hosted on Heroku. The free plan will let you host a website for free. The only downside for the free plan is that this app can “fall asleep”. Basically, if it remains inactive for 30 minutes, it will go to sleep. Any website launches that “wake-up” the app results in a long loading time.\nHeroku’s free plan allows me to host a web page via their web worker, and to have a background script running with a 2nd worker. It also comes with a free 10,000 rows in a PostgreSQL database. Both the 2nd worker and the database came in handy for the reporting aspect of this project.\n\n\nReporting\nI thought it would be nice to let the user have the ability to create a report based on their filters and current view. The basic workflow of the reporting is as follows:\n\nUser requests report, filter parameters & current view are recorded in the database\nBackground script makes request every 10 seconds to see if new reports have been requested\nIf new report is requested, a new report is created, and a message is sent back to the client that their report is ready\nReport is noted as “Ready to download” and the Word Document is sent to the client\n\nThere were more obvious avenues I could have taken for the reporting, but their convenience was outweighed by their potential failure (or cost). For example, I could have used something like Celery or Redis for managing task queues, but I think the costs would have been prohibitive (for a project that I am mostly trying to keep free). I could have also just processed the report within the Flask application, but if I did not return any result within 30 seconds, the application would crash.\nMy favorite part of the reporting logic is its use of CARTO and PostgreSQL/PostGIS. Seeing as how I could not load all of the json data from AWS in memory (I kept getting memory errors), I had to pull in the data from CARTO via their SQL API. Here is a bit of an example of what the query construction actually looks like:\n\nCARTO’s SQL API was quite helpful because it allowed me to pull in just summaries of the data, not all of it, thus avoiding any memory issues. It also takes advantage of PostGIS functions (like ST_WITHIN) to query cases inside of the user’s view.\nAgain, you can find this project on GitHub. Thank you for reading."
  },
  {
    "objectID": "posts/mapping-gondor-and-mordor/index.html",
    "href": "posts/mapping-gondor-and-mordor/index.html",
    "title": "Mapping Gondor and Mordor",
    "section": "",
    "text": "Middle Earth\n\n\nRobert Rose of William & Mary posted a very interesting analysis of an optimal route from the Shire to Mt. Doom. Thankfully, he also posted the data behind the analysis, which included some great 50m DEMs and feature classes.\nI’m not going to go over the process in great detail, but it involved the following:\n\nCreating a fixed layout in Pro where I could export data from\nCreating a shaded relief in Blender, following Daniel Huffman’s method\nBlending it all together in Photoshop, again following Daniel Huffman’s method and drawing inspiration from his Photoshop terrain walk-through\nLabeling in Illustrator. Thanks to Pete Klassen for making the Ringbearer typeface free to use (for personal use).\n\nI really wanted to include Barad-dûr (Sauron’s fave hangout), but it seemed strange to include something outside of natural features and political regions. Also, it would be interesting to see this with some bathymmetry data, but it sadly doesn’t exist in this dataset! There is another dataset that does include some contouring in the ocean, but its DEM is 200m and the two datasets sadly don’t line-up in any GIS software."
  },
  {
    "objectID": "posts/mesopotamian-marshes/index.html",
    "href": "posts/mesopotamian-marshes/index.html",
    "title": "Mesopotamian Marshes",
    "section": "",
    "text": "For millennia, the marshes in Mesopotamia — located by Iraq and Iran — boasted a rich culture of Marsh Arabs and a diverse ecosystem. Enter Saddam Hussein: after the building of dams and the targeting of marshes for political reasons, the ecosystem went under a drastic change. This project helps visualize the changes from 1988 to 2015 using Landsat 7 images and the remote sensing software ERDAS Imagine.\nImages were acquired via USGS’s Global Visualization Viewer (GloVis). These images mostly derived from NASA’s Landsat 7 satellite, though for the earlier year of 1988, the Landsat 5 satellite was used. My partner in this project, Kristen Childs, and I aimed to analyze images from the area’s wet season (February to March) in order to minimize potential errors in analysis. The seven bands in each of the raw images were then stacked together so the images could undergo supervised classification.\n\n\n\nLandsat 7 Capture (False Color)\n\n\nTo understand what type of land changed into another, we took five Areas of Interest (AOIs): Marsh, Agriculture/Light Vegetation, Water, Dead Marsh, and Barren Land. Each of these AOIs were captured using 30 samples from the original images for a total of 150 samples. From there we performed a supervised classification in which ERDAS color-codes the image based on our five classes of AOIs. The end result is a clearer picture of the area’s land use.\nThe AOIs had the following color-codes: - Marsh: Maroon - Dead Marsh: Tan - Barren Land: Light Yellow - Water: Blue - Green: Agriculture/Light Vegetation\n\n\n\nLand Use Change (1988-2015)\n\n\nIn order to get statistics on what percentage of land types changed to other land types, we used the software TerrSet.\n\n\n\nLand Use Change in Sq. Miles\n\n\nAs one can see, the impacts of humans were already apparent in 1988, albeit small. The year 2000 shows the ugly reality of Saddam Hussein’s infrastructure projects. Though marshes have reclaimed more ground as of 2015, it is not close to what it once was."
  },
  {
    "objectID": "posts/tanaka-contours-on-mars/index.html",
    "href": "posts/tanaka-contours-on-mars/index.html",
    "title": "Tanaka Contours on Mars",
    "section": "",
    "text": "Background\nTanaka contours, also known as illuminated contours, are a unique way of visualizing elevation. Tanaka Kitiro (developer of said method), used light from a northwesterly direction and varying line width & color based on the line’s angle.\n\n\n\nMap of Japan by Kitiro Tanaka (1950s)\n\n\nThis guide is an adaption of Anita Graser’s excellent guide to creating Tanaka contours in QGIS. Here, I will show how to accomplish the same thing in ArcGIS Pro (with some notes on data preparation).\n\n\nSelect a Location\nFirst, find an area where you want to create contours. For me it was Kasei Valles on Mars.\n\n\n\nKasei Valles\n\n\nIf you’re curious why Mars looks this way, I used John Nelson’s custom Imhof Style on a 200m DEM of Mars (from the USGS). Sure, Mars doesn’t have the lush landscape of Switzerland, but it looks pretty neat!\n\n\nClipping the Raster\nThis isn’t strictly necessary, but it will help speed up processing down the road. This can be accomplished via the Clip Raster tool in ArcGIS Pro, or through this GDAL command (in this instance, it said my source and target ellipsoids were not on the same celestial body, which was strange because both were in the same projection).\ngdalwarp -cutline clip.shp -crop\\_to\\_cutline mars.tif kasei.tif\n\n\n\n\n\nCreating Contours\nThis could be as easy as this GDAL command\ngdal\\_contour -a elev input.tif output.shp -i 200.0\nWhile this will create accurate contours, I’m looking for something smooth and aesthetically pleasing. In order to create smooth contours, the DEM needs to be generalized (this is possible with some more GDAL commands and maybe some Python, but I’m not going to go into that here).\nIn ArcGIS Pro, the DEM needs to be run through the Focal Statistics tool. Depending on the size of the DEM and the scale you are working with, your width and height parameters will be different. Play around with the settings until you find something appropriately smooth.\n\n\n\n\n\n\nIt’s pretty squid-like\n\n\nThe contours created with the smoothed DEM look a lot better than the ones created with the raw DEM. The contour interval for others will depend on the scale of your layout (mine is approximately 1:3 million).\n\n\n\n\n\n\nOriginal contours in red, smoothed in blue\n\n\n\n\nCalculating the Azimuth, Color, and Width\nSince line color and width is dependent on a line segment’s azimuth, the line needs to be split by its vertices. This can be accomplished through the Split Line at Vertices tool in Pro.\nAfter the contours have been split, I added three fields (all with the type float): azimuth, width, and height.\nFor the azimuth calculation, open up the field calculator on your split contours and call the following code block with get_azimuth(!Shape!) on the azimuth field:\n\nThis code was adapted from the QGIS function azimuth().\nFor the color calculation, create another field and call the following code block with get_color(!azimuth!):\n\nThis is to make it look like the sun is based in the north west (-45°).\nThe width calculation will be similar in concept. Line segments that are in the sun or shadow will be thicker, while those in the orthogonal direction (meaning at right angles) will be thinner. Call the following code block with get_width(!azimuth!)\n\n\n\nStyling the Map\nClick on the processed contours, select the Appearance tab, and set the Layer Blend to Overlay.\nWith the contours still selected, go to the Symbology pane, navigate to Vary symbology by attribute, and set the Color field to color and the Size field to width.\n\n\n\nThe default for the color ramp is acceptable, but in the Size area, check the Enable size range checkbox, and set the range from 1pt to 2 pt (or however thick you would like your lines).\n\n\n\nThen, assuming your blurred DEM is still the back-drop, your map should look like this:\n\n\n\nMess around with the color ramp on your DEM to get the look you’re going for. I tried one with an “earthy” look to it:\n\n\n\nThat’s all there is to it. This workflow can be repeated anywhere and adjusted to your liking. Thanks for reading."
  },
  {
    "objectID": "posts/the-tour-de-iberia/index.html",
    "href": "posts/the-tour-de-iberia/index.html",
    "title": "The Tour de Iberia",
    "section": "",
    "text": "The Fictional Tour de Iberia\n\n\nThis is my attempt at creating a fictional bike route, akin to the Tour de France. While trying to figure out the most punishing route through the Pyrenees was enjoyable, the best part of this project was visualizing the Iberian Peninsula’s beautiful terrain.\nThe terrain itself consists of three main components: a hillshade, simplified land-use, and water. Since the terrain is my favorite part, I made another version of this map just showcasing the natural beauty:\n\n\n\n\nThe Fictional Tour de Iberia\n\n\nThe data preparation was done with ArcGIS Pro & GDAL, shaded relief in Blender, overlay of simplified land use and water done in Photoshop, and labeling finalized in Illustrator.\nI wish I had kept the elevation data outside of the Iberian peninsula (e.g. Algeria, Morocco, Balearic Islands, France), but I clipped it all early in the process.\nSources/Inspirations:\n\nCreating Shaded Relief in Blender - Daniel Huffman\nAdding Shaded Relief in Photoshop - Daniel Huffman\nTerrain in Photoshop Walkthrough - Daniel Huffman\nEU DEM 25m\nEU Land Cover 250M\nVector Data for 1:10m scale - Natural Earth\nColor inspirations for land use come from Tom Patterson’s maps."
  },
  {
    "objectID": "posts/varieties-of-democracy/index.html",
    "href": "posts/varieties-of-democracy/index.html",
    "title": "Varieties of Democracy",
    "section": "",
    "text": "See the project here. ## Overview The Varieties of Democracy (V-Dem) is an attempt to measure and conceptualize democracy throughout the world. With a global team of over 50 social scientists, they put forth five high-level indicators of democracy (scale 0-1, low-high):\nThis was built by me, Ryan Cooper, and Kristina Randrup."
  },
  {
    "objectID": "posts/varieties-of-democracy/index.html#tech-stack",
    "href": "posts/varieties-of-democracy/index.html#tech-stack",
    "title": "Varieties of Democracy",
    "section": "Tech stack",
    "text": "Tech stack\nData was prepared in Python. Most of the vizualition is done with Leaflet, but if you choose to view country-level data by population (under the advanced menu), then the application utilizes d3.\nSources: - V-Dem datasets"
  },
  {
    "objectID": "posts/visualizing-democratic-backsliding/index.html",
    "href": "posts/visualizing-democratic-backsliding/index.html",
    "title": "Visualizing Democratic Backsliding",
    "section": "",
    "text": "Very short write-up here. This was created with data from the V-Dem institute, and was inspired by their article on toxic polarization and democratic backsliding.\nSnippet from the final layout\nData processing, map creation, and chart creation was all done in Python (thanks to geopandas and Plotly). Final layout was created in Illustrator.\nQuick note on the methodology. While the high-level democracy indicators are on a scale of 0-1, the societal polarization indicator was originally on a scale of 1-4 (where 4 is no societal polarization, and 1 is high polarization). I normalized this to match the 0-1 scale, and then took the inverse of that scale, so that a higher value meant a higher level of societal polarization."
  },
  {
    "objectID": "posts/visualizing-the-riot-on-the-capitol/index.html",
    "href": "posts/visualizing-the-riot-on-the-capitol/index.html",
    "title": "Visualizing the Riot on the Capitol",
    "section": "",
    "text": "The Hack\nIn the aftermath of the attempted insurrection, Parler faced a flurry of events. Google and Apple dropped them from their respective app stores, and Amazon informed them that they would be shutting down their servers. Oh, and someone (@donk_enby) breached their website and extracted 56.7 TB of user data.\n\n\nThe Data\nAfter the website breach, Kyle McDonald made the data available in an easily digestible CSV, alongside instructions on how to view the videos for each data point.\nWhile this project is not interested in finding out who was at the riot, or in viewing any of the videos, you can check out other projects, such as @patr10tic’s map which visualize videos the day of the riot.\n\n\n\n\nMy Process\nThis was my first foray into R. I probably could have saved myself some time and did this in Python instead, but I’ve been wanting to pickup R for a while now. All that to say, please be kind, I am no R expert, and I’m sure there was a faster/easier way to do this, say via ggplot or ggmap. I also could not have done this without two StackExchange posts on saving Leaflet maps as PNGs and combining PNGs into a GIF.\nFirst, I read in the CSV (which I had already converted from GMT timestamps to EST), filter it for the date I want, convert it into a simple features object (via the sf library), and then only select the points within the Capitol area of D.C.\n\nFrom there, I loop over the all the timestamps, select the pertinent data, create a map from said data, and repeat. More is explained in the code comments.\n\nThe following is the createMap() function which is what utilizes Leaflet for R in making the maps.\n\nAfter all of that, I get a list of the PNG file outputs and combine them into a GIF.\n\nThat’s all, folks."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  }
]